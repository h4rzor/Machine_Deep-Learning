{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Card Fraud Detection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgfcJK50TJif",
        "colab_type": "code",
        "outputId": "a2411c17-399e-4da1-bfce-42bbb13f14f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense\n",
        "from keras import regularizers\n",
        "%matplotlib inline\n",
        "#Importing the libraries I am goign to use "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BT34AqHTNJU",
        "colab_type": "code",
        "outputId": "451fc4d4-57d5-47e7-bd53-2220060771dd",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#I have to upload the file to google colab in order to use it afterwards"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4843aa89-24bc-4c3e-bdd0-4f12b94e2d58\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4843aa89-24bc-4c3e-bdd0-4f12b94e2d58\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving creditcard.csv to creditcard.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tegj4G2TYEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "df = pd.read_csv(io.BytesIO(uploaded['creditcard.csv']))\n",
        "#Using pandas to read the csv file."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejITA5jTdyKO",
        "colab_type": "code",
        "outputId": "8bfe32dd-f946-4c33-a1c9-c6edf560a27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>172786.0</td>\n",
              "      <td>-11.881118</td>\n",
              "      <td>10.071785</td>\n",
              "      <td>-9.834783</td>\n",
              "      <td>-2.066656</td>\n",
              "      <td>-5.364473</td>\n",
              "      <td>-2.606837</td>\n",
              "      <td>-4.918215</td>\n",
              "      <td>7.305334</td>\n",
              "      <td>1.914428</td>\n",
              "      <td>4.356170</td>\n",
              "      <td>-1.593105</td>\n",
              "      <td>2.711941</td>\n",
              "      <td>-0.689256</td>\n",
              "      <td>4.626942</td>\n",
              "      <td>-0.924459</td>\n",
              "      <td>1.107641</td>\n",
              "      <td>1.991691</td>\n",
              "      <td>0.510632</td>\n",
              "      <td>-0.682920</td>\n",
              "      <td>1.475829</td>\n",
              "      <td>0.213454</td>\n",
              "      <td>0.111864</td>\n",
              "      <td>1.014480</td>\n",
              "      <td>-0.509348</td>\n",
              "      <td>1.436807</td>\n",
              "      <td>0.250034</td>\n",
              "      <td>0.943651</td>\n",
              "      <td>0.823731</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>172787.0</td>\n",
              "      <td>-0.732789</td>\n",
              "      <td>-0.055080</td>\n",
              "      <td>2.035030</td>\n",
              "      <td>-0.738589</td>\n",
              "      <td>0.868229</td>\n",
              "      <td>1.058415</td>\n",
              "      <td>0.024330</td>\n",
              "      <td>0.294869</td>\n",
              "      <td>0.584800</td>\n",
              "      <td>-0.975926</td>\n",
              "      <td>-0.150189</td>\n",
              "      <td>0.915802</td>\n",
              "      <td>1.214756</td>\n",
              "      <td>-0.675143</td>\n",
              "      <td>1.164931</td>\n",
              "      <td>-0.711757</td>\n",
              "      <td>-0.025693</td>\n",
              "      <td>-1.221179</td>\n",
              "      <td>-1.545556</td>\n",
              "      <td>0.059616</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.924384</td>\n",
              "      <td>0.012463</td>\n",
              "      <td>-1.016226</td>\n",
              "      <td>-0.606624</td>\n",
              "      <td>-0.395255</td>\n",
              "      <td>0.068472</td>\n",
              "      <td>-0.053527</td>\n",
              "      <td>24.79</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>1.919565</td>\n",
              "      <td>-0.301254</td>\n",
              "      <td>-3.249640</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>2.630515</td>\n",
              "      <td>3.031260</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.708417</td>\n",
              "      <td>0.432454</td>\n",
              "      <td>-0.484782</td>\n",
              "      <td>0.411614</td>\n",
              "      <td>0.063119</td>\n",
              "      <td>-0.183699</td>\n",
              "      <td>-0.510602</td>\n",
              "      <td>1.329284</td>\n",
              "      <td>0.140716</td>\n",
              "      <td>0.313502</td>\n",
              "      <td>0.395652</td>\n",
              "      <td>-0.577252</td>\n",
              "      <td>0.001396</td>\n",
              "      <td>0.232045</td>\n",
              "      <td>0.578229</td>\n",
              "      <td>-0.037501</td>\n",
              "      <td>0.640134</td>\n",
              "      <td>0.265745</td>\n",
              "      <td>-0.087371</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.026561</td>\n",
              "      <td>67.88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>-0.240440</td>\n",
              "      <td>0.530483</td>\n",
              "      <td>0.702510</td>\n",
              "      <td>0.689799</td>\n",
              "      <td>-0.377961</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>-0.686180</td>\n",
              "      <td>0.679145</td>\n",
              "      <td>0.392087</td>\n",
              "      <td>-0.399126</td>\n",
              "      <td>-1.933849</td>\n",
              "      <td>-0.962886</td>\n",
              "      <td>-1.042082</td>\n",
              "      <td>0.449624</td>\n",
              "      <td>1.962563</td>\n",
              "      <td>-0.608577</td>\n",
              "      <td>0.509928</td>\n",
              "      <td>1.113981</td>\n",
              "      <td>2.897849</td>\n",
              "      <td>0.127434</td>\n",
              "      <td>0.265245</td>\n",
              "      <td>0.800049</td>\n",
              "      <td>-0.163298</td>\n",
              "      <td>0.123205</td>\n",
              "      <td>-0.569159</td>\n",
              "      <td>0.546668</td>\n",
              "      <td>0.108821</td>\n",
              "      <td>0.104533</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>172792.0</td>\n",
              "      <td>-0.533413</td>\n",
              "      <td>-0.189733</td>\n",
              "      <td>0.703337</td>\n",
              "      <td>-0.506271</td>\n",
              "      <td>-0.012546</td>\n",
              "      <td>-0.649617</td>\n",
              "      <td>1.577006</td>\n",
              "      <td>-0.414650</td>\n",
              "      <td>0.486180</td>\n",
              "      <td>-0.915427</td>\n",
              "      <td>-1.040458</td>\n",
              "      <td>-0.031513</td>\n",
              "      <td>-0.188093</td>\n",
              "      <td>-0.084316</td>\n",
              "      <td>0.041333</td>\n",
              "      <td>-0.302620</td>\n",
              "      <td>-0.660377</td>\n",
              "      <td>0.167430</td>\n",
              "      <td>-0.256117</td>\n",
              "      <td>0.382948</td>\n",
              "      <td>0.261057</td>\n",
              "      <td>0.643078</td>\n",
              "      <td>0.376777</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.473649</td>\n",
              "      <td>-0.818267</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>0.013649</td>\n",
              "      <td>217.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>284807 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time         V1         V2  ...       V28  Amount  Class\n",
              "0            0.0  -1.359807  -0.072781  ... -0.021053  149.62      0\n",
              "1            0.0   1.191857   0.266151  ...  0.014724    2.69      0\n",
              "2            1.0  -1.358354  -1.340163  ... -0.059752  378.66      0\n",
              "3            1.0  -0.966272  -0.185226  ...  0.061458  123.50      0\n",
              "4            2.0  -1.158233   0.877737  ...  0.215153   69.99      0\n",
              "...          ...        ...        ...  ...       ...     ...    ...\n",
              "284802  172786.0 -11.881118  10.071785  ...  0.823731    0.77      0\n",
              "284803  172787.0  -0.732789  -0.055080  ... -0.053527   24.79      0\n",
              "284804  172788.0   1.919565  -0.301254  ... -0.026561   67.88      0\n",
              "284805  172788.0  -0.240440   0.530483  ...  0.104533   10.00      0\n",
              "284806  172792.0  -0.533413  -0.189733  ...  0.013649  217.00      0\n",
              "\n",
              "[284807 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDlBOz14IGML",
        "colab_type": "text"
      },
      "source": [
        "This is our dataset. As you can see it consist of 284807 rows and 31 columns. Due to privacy measures the columns names are these. We have a column Time which is the time the transaction is made. We have two more columns that are important. The amount of the transaction and the Class column - whether the transaction is a fraud or not a fraud. We are trying to predict just that using all the columns except the time column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjdWyf9Pd0EH",
        "colab_type": "code",
        "outputId": "8cdda178-e486-4609-c1e8-877cc42580a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "df.isnull().any() #Checking whether we have a NaN "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      False\n",
              "V1        False\n",
              "V2        False\n",
              "V3        False\n",
              "V4        False\n",
              "V5        False\n",
              "V6        False\n",
              "V7        False\n",
              "V8        False\n",
              "V9        False\n",
              "V10       False\n",
              "V11       False\n",
              "V12       False\n",
              "V13       False\n",
              "V14       False\n",
              "V15       False\n",
              "V16       False\n",
              "V17       False\n",
              "V18       False\n",
              "V19       False\n",
              "V20       False\n",
              "V21       False\n",
              "V22       False\n",
              "V23       False\n",
              "V24       False\n",
              "V25       False\n",
              "V26       False\n",
              "V27       False\n",
              "V28       False\n",
              "Amount    False\n",
              "Class     False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Pdx7IMd6uV",
        "colab_type": "code",
        "outputId": "a3d8ae17-a3c0-441c-9765-7894bd9b1565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "count_classes = df['Class'].value_counts().plot(kind = 'bar')\n",
        "#There is a huge disbalance in the dataset and I do not think I can even see the frauds."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD1CAYAAAClSgmzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPOklEQVR4nO3cX4jdZ53H8fdnEyuyrjba2dBN0k3R\nLEsUNmpoA+6FayFNuxepUEt7YUMJRjAFBS+M3kTUgl5ooaCBSENTcY2lKg270RhiF5GlNVMtbdNu\nN0NttwmxjU1sXUTd1u9ezBM8nT3PzDR/zkmT9wt+nN/5Pn9+z4FhPpzf75lJVSFJ0jB/Me4FSJLO\nXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6Fo57AWfaJZdcUsuXLx/3MiTpdeWhhx76dVVNzKyfdyGx\nfPlyJicnx70MSXpdSfLMsLq3myRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqOu/+\nmO71YvmWfxv3Es4rT3/pn8e9BOm85DcJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhI\nkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSp\ny5CQJHXNGRJJliW5P8njSQ4m+USrfy7JkSQPt+PagTGfSTKV5MkkVw/U17XaVJItA/XLkzzY6t9J\nclGrv7G9n2rty8/kh5ckzW4+3yReBj5VVSuBNcDmJCtb2+1VtaodewBa243Au4B1wNeTLEiyAPga\ncA2wErhpYJ4vt7neCZwANrb6RuBEq9/e+kmSRmTOkKiqo1X183b+W+AJYMksQ9YDu6rqD1X1S2AK\nuKIdU1X1VFX9EdgFrE8S4IPAvW38TuC6gbl2tvN7gataf0nSCLymZxLtds97gAdb6dYkjyTZkWRR\nqy0Bnh0YdrjVevW3A7+pqpdn1F81V2t/sfWfua5NSSaTTB47duy1fCRJ0izmHRJJ3gx8F/hkVb0E\nbAPeAawCjgJfOSsrnIeq2l5Vq6tq9cTExLiWIUnnnXmFRJI3MB0Q36qq7wFU1XNV9UpV/Qn4BtO3\nkwCOAMsGhi9ttV79BeDiJAtn1F81V2t/a+svSRqB+exuCnAn8ERVfXWgfulAtw8Bj7Xz3cCNbWfS\n5cAK4GfAAWBF28l0EdMPt3dXVQH3A9e38RuA+wbm2tDOrwd+3PpLkkZg4dxdeD/wEeDRJA+32meZ\n3p20CijgaeBjAFV1MMk9wONM74zaXFWvACS5FdgLLAB2VNXBNt+ngV1Jvgj8gulQor1+M8kUcJzp\nYJEkjcicIVFVPwWG7SjaM8uY24DbhtT3DBtXVU/x59tVg/XfAx+ea42SpLPDv7iWJHUZEpKkLkNC\nktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJ\nXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktQ1\nZ0gkWZbk/iSPJzmY5BOt/rYk+5Icaq+LWj1J7kgyleSRJO8dmGtD638oyYaB+vuSPNrG3JEks11D\nkjQa8/km8TLwqapaCawBNidZCWwB9lfVCmB/ew9wDbCiHZuAbTD9Cx/YClwJXAFsHfilvw346MC4\nda3eu4YkaQTmDImqOlpVP2/nvwWeAJYA64GdrdtO4Lp2vh64u6Y9AFyc5FLgamBfVR2vqhPAPmBd\na3tLVT1QVQXcPWOuYdeQJI3Aa3omkWQ58B7gQWBxVR1tTb8CFrfzJcCzA8MOt9ps9cND6sxyDUnS\nCMw7JJK8Gfgu8MmqemmwrX0DqDO8tleZ7RpJNiWZTDJ57Nixs7kMSbqgzCskkryB6YD4VlV9r5Wf\na7eKaK/Pt/oRYNnA8KWtNlt96ZD6bNd4laraXlWrq2r1xMTEfD6SJGke5rO7KcCdwBNV9dWBpt3A\nyR1KG4D7Buo3t11Oa4AX2y2jvcDaJIvaA+u1wN7W9lKSNe1aN8+Ya9g1JEkjsHAefd4PfAR4NMnD\nrfZZ4EvAPUk2As8AN7S2PcC1wBTwO+AWgKo6nuQLwIHW7/NVdbydfxy4C3gT8IN2MMs1JEkjMGdI\nVNVPgXSarxrSv4DNnbl2ADuG1CeBdw+pvzDsGpKk0fAvriVJXYaEJKnLkJAkdRkSkqQuQ0KS1GVI\nSJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQk\nqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdc0ZEkl2JHk+yWMDtc8l\nOZLk4XZcO9D2mSRTSZ5McvVAfV2rTSXZMlC/PMmDrf6dJBe1+hvb+6nWvvxMfWhJ0vzM55vEXcC6\nIfXbq2pVO/YAJFkJ3Ai8q435epIFSRYAXwOuAVYCN7W+AF9uc70TOAFsbPWNwIlWv731kySN0Jwh\nUVU/AY7Pc771wK6q+kNV/RKYAq5ox1RVPVVVfwR2AeuTBPggcG8bvxO4bmCune38XuCq1l+SNCKn\n80zi1iSPtNtRi1ptCfDsQJ/Drdarvx34TVW9PKP+qrla+4utvyRpRE41JLYB7wBWAUeBr5yxFZ2C\nJJuSTCaZPHbs2DiXIknnlVMKiap6rqpeqao/Ad9g+nYSwBFg2UDXpa3Wq78AXJxk4Yz6q+Zq7W9t\n/YetZ3tVra6q1RMTE6fykSRJQ5xSSCS5dODth4CTO592Aze2nUmXAyuAnwEHgBVtJ9NFTD/c3l1V\nBdwPXN/GbwDuG5hrQzu/Hvhx6y9JGpGFc3VI8m3gA8AlSQ4DW4EPJFkFFPA08DGAqjqY5B7gceBl\nYHNVvdLmuRXYCywAdlTVwXaJTwO7knwR+AVwZ6vfCXwzyRTTD85vPO1PK0l6TeYMiaq6aUj5ziG1\nk/1vA24bUt8D7BlSf4o/364arP8e+PBc65MknT3+xbUkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlS\nlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZ\nEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUNWdIJNmR5Pkkjw3U\n3pZkX5JD7XVRqyfJHUmmkjyS5L0DYza0/oeSbBiovy/Jo23MHUky2zUkSaMzn28SdwHrZtS2APur\nagWwv70HuAZY0Y5NwDaY/oUPbAWuBK4Atg780t8GfHRg3Lo5riFJGpE5Q6KqfgIcn1FeD+xs5zuB\n6wbqd9e0B4CLk1wKXA3sq6rjVXUC2Aesa21vqaoHqqqAu2fMNewakqQROdVnEour6mg7/xWwuJ0v\nAZ4d6He41WarHx5Sn+0akqQROe0H1+0bQJ2BtZzyNZJsSjKZZPLYsWNncymSdEE51ZB4rt0qor0+\n3+pHgGUD/Za22mz1pUPqs13j/6mq7VW1uqpWT0xMnOJHkiTNdKohsRs4uUNpA3DfQP3mtstpDfBi\nu2W0F1ibZFF7YL0W2NvaXkqypu1qunnGXMOuIUkakYVzdUjybeADwCVJDjO9S+lLwD1JNgLPADe0\n7nuAa4Ep4HfALQBVdTzJF4ADrd/nq+rkw/CPM72D6k3AD9rBLNeQJI3InCFRVTd1mq4a0reAzZ15\ndgA7htQngXcPqb8w7BqSpNHxL64lSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIk\nJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS\n1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHWdVkgkeTrJo0keTjLZam9Lsi/Jofa6qNWT5I4k\nU0keSfLegXk2tP6HkmwYqL+vzT/VxuZ01itJem3OxDeJf6qqVVW1ur3fAuyvqhXA/vYe4BpgRTs2\nAdtgOlSArcCVwBXA1pPB0vp8dGDcujOwXknSPJ2N203rgZ3tfCdw3UD97pr2AHBxkkuBq4F9VXW8\nqk4A+4B1re0tVfVAVRVw98BckqQRON2QKOBHSR5KsqnVFlfV0Xb+K2BxO18CPDsw9nCrzVY/PKQu\nSRqRhac5/h+r6kiSvwb2JfnPwcaqqiR1mteYUwuoTQCXXXbZ2b6cJF0wTuubRFUdaa/PA99n+pnC\nc+1WEe31+db9CLBsYPjSVputvnRIfdg6tlfV6qpaPTExcTofSZI04JRDIslfJvmrk+fAWuAxYDdw\ncofSBuC+dr4buLntcloDvNhuS+0F1iZZ1B5YrwX2traXkqxpu5puHphLkjQCp3O7aTHw/bYrdSHw\nL1X1wyQHgHuSbASeAW5o/fcA1wJTwO+AWwCq6niSLwAHWr/PV9Xxdv5x4C7gTcAP2iFJGpFTDomq\negr4hyH1F4CrhtQL2NyZawewY0h9Enj3qa5RknR6/ItrSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKS\npC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnq\nMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqeucD4kk65I8mWQq\nyZZxr0eSLiTndEgkWQB8DbgGWAnclGTleFclSReOczokgCuAqap6qqr+COwC1o95TZJ0wVg47gXM\nYQnw7MD7w8CVMzsl2QRsam//J8mTI1jbheIS4NfjXsRc8uVxr0Bj8Lr42Xwd+dthxXM9JOalqrYD\n28e9jvNRksmqWj3udUgz+bM5Guf67aYjwLKB90tbTZI0Aud6SBwAViS5PMlFwI3A7jGvSZIuGOf0\n7aaqejnJrcBeYAGwo6oOjnlZFxpv4+lc5c/mCKSqxr0GSdI56ly/3SRJGiNDQpLUZUhIkrrO6QfX\nGq0kf8/0X7QvaaUjwO6qemJ8q5I0Tn6TEABJPs30vz0J8LN2BPi2/1hR57Ikt4x7DeczdzcJgCT/\nBbyrqv53Rv0i4GBVrRjPyqTZJfnvqrps3Os4X3m7SSf9Cfgb4JkZ9UtbmzQ2SR7pNQGLR7mWC40h\noZM+CexPcog//1PFy4B3AreObVXStMXA1cCJGfUA/zH65Vw4DAkBUFU/TPJ3TP979sEH1weq6pXx\nrUwC4F+BN1fVwzMbkvz76Jdz4fCZhCSpy91NkqQuQ0KS1GVISJK6DAlJUpchIUnq+j+QLFLiMO0n\nlAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Lllf1PeUQN",
        "colab_type": "code",
        "outputId": "4b205fcc-e064-451b-faee-6075c8891db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "df['Class'].value_counts() #This is a little better. The overwhelming majority of the dataset are not frauds."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    284315\n",
              "1       492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKkF8yP8epzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frauds = df[df['Class']==1] #This is the whole dataset where the Class column is equal to 1 - fraud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqh1sGNgeuJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normal = df[df['Class']==0] #This is also the whole dataset with all the columns where the Class column is equal to 0 - fraud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tiK0kyMeu02",
        "colab_type": "code",
        "outputId": "97f98e99-ce21-40f1-c3bc-1ca45e28391d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "frauds.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(492, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6j47kt2e5CM",
        "colab_type": "code",
        "outputId": "987e2110-0da4-43cf-f39f-a4edabcbd20d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "normal.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(284315, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5X91xi-e53f",
        "colab_type": "code",
        "outputId": "27fba187-64b4-446e-8b14-8970fad084b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "frauds['Amount'].describe() #Just some parameters for the frauds"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     492.000000\n",
              "mean      122.211321\n",
              "std       256.683288\n",
              "min         0.000000\n",
              "25%         1.000000\n",
              "50%         9.250000\n",
              "75%       105.890000\n",
              "max      2125.870000\n",
              "Name: Amount, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm3JGf1Ne9qN",
        "colab_type": "code",
        "outputId": "0e70a1d2-e891-44c3-8040-721ced2ca8fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "normal['Amount'].describe() #Parameters for the normal transcations"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    284315.000000\n",
              "mean         88.291022\n",
              "std         250.105092\n",
              "min           0.000000\n",
              "25%           5.650000\n",
              "50%          22.000000\n",
              "75%          77.050000\n",
              "max       25691.160000\n",
              "Name: Amount, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj9Gan5AfEz8",
        "colab_type": "code",
        "outputId": "f5a52f58-14c3-4846-cf79-971438706d77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "f, (ax1, ax2) = plt.subplots(2,1,sharex=True)\n",
        "f.suptitle('Amount')\n",
        "ax1.hist(frauds['Amount'],bins=50)\n",
        "ax1.set_title('frauds')\n",
        "ax2.hist(normal['Amount'],bins=50)\n",
        "ax2.set_title('normal')\n",
        "plt.xlim((0, 20000))\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Amount ($)')\n",
        "plt.ylabel('Number of Transactions')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Number of Transactions')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEjCAYAAAAVCvdtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMTOzLJxQzMwsCycUMzPLwgnFzMyy+C/G3AU4rVI+FQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Wijh1zWIUz",
        "colab_type": "text"
      },
      "source": [
        "Here I am plotting the Amount and the number of the transactions made in the datast. The frauds are less than the normal transactions but I do not see any correlation at this time, we will see what the algorithm say."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOZ1SIsFfpDD",
        "colab_type": "code",
        "outputId": "38b0dafd-186d-4dcd-fbe9-b23ed7fb8c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "f, (ax1,ax2) = plt.subplots(2,1,sharex=True)\n",
        "f.suptitle('During what time')\n",
        "\n",
        "ax1.scatter(frauds.Time, frauds.Amount)\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "ax2.scatter(normal.Time, normal.Amount)\n",
        "ax2.set_title('Normal')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Normal')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEVCAYAAAD6u3K7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQIpL/D4nMJGlZ7KtWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETP-6ZonWm8J",
        "colab_type": "text"
      },
      "source": [
        "Here I scattered the dataset in such manner that the dots are dependant on the time the transaction is made and the amount of the transaction. It is hard to tell if there is some correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTYOeUUFgdhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler #Importing the scaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82KXzGHyg363",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df.drop('Time',axis='columns') #I am dropping the time axis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pUbXvCjg_Fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['Amount'] = StandardScaler().fit_transform(train['Amount'].values.reshape(-1,1)) #Scaling the Amount column in order the algorithm to learn faster and more accurately"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri-bzUw9hLIN",
        "colab_type": "code",
        "outputId": "8fdcc5e0-0628-4b70-daef-40bf00471c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "train['Amount']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         0.244964\n",
              "1        -0.342475\n",
              "2         1.160686\n",
              "3         0.140534\n",
              "4        -0.073403\n",
              "            ...   \n",
              "284802   -0.350151\n",
              "284803   -0.254117\n",
              "284804   -0.081839\n",
              "284805   -0.313249\n",
              "284806    0.514355\n",
              "Name: Amount, Length: 284807, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhHGN7B2hRa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split #Helping to split the dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02AQdFbEhwmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test = train_test_split(train,test_size=.2) #We are splitting the dataset into 80% train and 20%test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fiFrWV5h1Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train[X_train['Class']==0] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUYgTQBTXk7k",
        "colab_type": "text"
      },
      "source": [
        "We are going to train the algorithm only with the true values. In other words only the not fraudulent transactions will be used. When we make a prediction whether the transaction is fraud, if the mistake(MSE) is small there is high change that the transaction is not fraud. And if the loss(MSE) is high -> the transaction is a fraud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvhqshy4iD_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.drop(['Class'],axis=1) #Dropping the class column, we do not need it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtK02TLSiL9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = X_test['Class'] #Just making the y_test variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4XENGfRiMoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = X_test.drop(['Class'], axis=1) #Dropping the class column from the test set also"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcZByHyeiX4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.values #Getting just the values without the column's names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfdRR9YDiawP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = X_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDwCY4wridhH",
        "colab_type": "code",
        "outputId": "8cdd98f9-6458-47d0-b55c-4d2c7146a125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(227447, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQUz0az8iesZ",
        "colab_type": "code",
        "outputId": "09b0be8c-417f-41ee-c983-950f2e60d981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "input_layer = Input(shape=(X_train.shape[1],)) #This is the input layer..sort of. We are feeding the model with the 29 columns left."
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15r0e5fgit4o",
        "colab_type": "code",
        "outputId": "2cd88cab-63fc-4f31-a564-065e4b8cb4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "encoder = Dense(units=14,activation='sigmoid')(input_layer)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPrMtSHyi1Ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Dense(units=7,activation='relu')(encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG5hoel1jkUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Dense(units=7,activation='sigmoid')((encoder))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPXk6RSwjuVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Dense(units=(X_train.shape[1]),activation='relu')(decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWka-jgbQQP",
        "colab_type": "text"
      },
      "source": [
        "This model learn by reducing the dimensionality of the input. As you can see we have almost 30 input features and as early as the second layer they become less. This is how the encoder works. It essentially encodes the input.The activations in these layers are sigmoid and relu. They are both frequently used. As the layers of the encoder goes deeper, the units parameter are getting less. We have to put the brackets after the Dense layer in order to tell tensorflow in which way do these layers move. The last ayer is the same as the input one as it is trying to predict the actual input we gave it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A_5p41RkPnc",
        "colab_type": "code",
        "outputId": "6edb6df5-ca1b-4634-c63a-216fe258590c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "autoencoder = Model(inputs=input_layer,output=decoder) #This is the model"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdJLtc4QnLDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.keras.losses.MSE #The loss in this case is Mean Squared Error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTWu7cBSj378",
        "colab_type": "code",
        "outputId": "ff958675-27d8-4999-8df3-770273fc5bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "nb_epochs = 100\n",
        "batch_size=32\n",
        "\n",
        "autoencoder.compile(optimizer='adam',loss=loss,metrics=['accuracy'])\n",
        "#Setting some parameters and optimizers"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoQu9Px7kkE5",
        "colab_type": "code",
        "outputId": "d0e23ce3-27aa-43b8-c120-2d0509cf0cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = autoencoder.fit(X_train,X_train,batch_size=batch_size,epochs=nb_epochs,verbose=1,shuffle=True,validation_data=(X_test,X_test))\n",
        "#We fit the autoencoder with X_train and shuffle it just in case, the validation data is X_test"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 227447 samples, validate on 56962 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "227447/227447 [==============================] - 11s 47us/step - loss: 0.9391 - acc: 0.2794 - val_loss: 0.9310 - val_acc: 0.3360\n",
            "Epoch 2/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8885 - acc: 0.3511 - val_loss: 0.9102 - val_acc: 0.3639\n",
            "Epoch 3/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8744 - acc: 0.3709 - val_loss: 0.9003 - val_acc: 0.3775\n",
            "Epoch 4/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8672 - acc: 0.3792 - val_loss: 0.8947 - val_acc: 0.3825\n",
            "Epoch 5/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8625 - acc: 0.3834 - val_loss: 0.8908 - val_acc: 0.3863\n",
            "Epoch 6/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8585 - acc: 0.3858 - val_loss: 0.8869 - val_acc: 0.3876\n",
            "Epoch 7/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8553 - acc: 0.3875 - val_loss: 0.8840 - val_acc: 0.3876\n",
            "Epoch 8/100\n",
            "227447/227447 [==============================] - 11s 47us/step - loss: 0.8527 - acc: 0.3894 - val_loss: 0.8815 - val_acc: 0.3925\n",
            "Epoch 9/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8500 - acc: 0.3919 - val_loss: 0.8750 - val_acc: 0.3928\n",
            "Epoch 10/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8369 - acc: 0.4006 - val_loss: 0.8598 - val_acc: 0.4121\n",
            "Epoch 11/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8271 - acc: 0.4123 - val_loss: 0.8546 - val_acc: 0.4099\n",
            "Epoch 12/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8234 - acc: 0.4136 - val_loss: 0.8515 - val_acc: 0.4153\n",
            "Epoch 13/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8215 - acc: 0.4132 - val_loss: 0.8498 - val_acc: 0.4153\n",
            "Epoch 14/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8203 - acc: 0.4135 - val_loss: 0.8493 - val_acc: 0.4148\n",
            "Epoch 15/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8194 - acc: 0.4142 - val_loss: 0.8476 - val_acc: 0.4162\n",
            "Epoch 16/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8185 - acc: 0.4156 - val_loss: 0.8466 - val_acc: 0.4184\n",
            "Epoch 17/100\n",
            "227447/227447 [==============================] - 10s 42us/step - loss: 0.8173 - acc: 0.4185 - val_loss: 0.8453 - val_acc: 0.4231\n",
            "Epoch 18/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8163 - acc: 0.4214 - val_loss: 0.8449 - val_acc: 0.4182\n",
            "Epoch 19/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8156 - acc: 0.4217 - val_loss: 0.8437 - val_acc: 0.4211\n",
            "Epoch 20/100\n",
            "227447/227447 [==============================] - 10s 45us/step - loss: 0.8149 - acc: 0.4218 - val_loss: 0.8434 - val_acc: 0.4247\n",
            "Epoch 21/100\n",
            "227447/227447 [==============================] - 10s 45us/step - loss: 0.8144 - acc: 0.4236 - val_loss: 0.8428 - val_acc: 0.4275\n",
            "Epoch 22/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8138 - acc: 0.4240 - val_loss: 0.8423 - val_acc: 0.4265\n",
            "Epoch 23/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8133 - acc: 0.4244 - val_loss: 0.8419 - val_acc: 0.4266\n",
            "Epoch 24/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8130 - acc: 0.4245 - val_loss: 0.8413 - val_acc: 0.4268\n",
            "Epoch 25/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8128 - acc: 0.4250 - val_loss: 0.8408 - val_acc: 0.4289\n",
            "Epoch 26/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8123 - acc: 0.4265 - val_loss: 0.8406 - val_acc: 0.4300\n",
            "Epoch 27/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8119 - acc: 0.4289 - val_loss: 0.8407 - val_acc: 0.4326\n",
            "Epoch 28/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8114 - acc: 0.4313 - val_loss: 0.8397 - val_acc: 0.4312\n",
            "Epoch 29/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8110 - acc: 0.4327 - val_loss: 0.8396 - val_acc: 0.4352\n",
            "Epoch 30/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8106 - acc: 0.4334 - val_loss: 0.8389 - val_acc: 0.4370\n",
            "Epoch 31/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8103 - acc: 0.4340 - val_loss: 0.8383 - val_acc: 0.4358\n",
            "Epoch 32/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8100 - acc: 0.4342 - val_loss: 0.8383 - val_acc: 0.4361\n",
            "Epoch 33/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8097 - acc: 0.4348 - val_loss: 0.8379 - val_acc: 0.4367\n",
            "Epoch 34/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8096 - acc: 0.4351 - val_loss: 0.8374 - val_acc: 0.4385\n",
            "Epoch 35/100\n",
            "227447/227447 [==============================] - 10s 42us/step - loss: 0.8093 - acc: 0.4358 - val_loss: 0.8376 - val_acc: 0.4395\n",
            "Epoch 36/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8091 - acc: 0.4357 - val_loss: 0.8374 - val_acc: 0.4389\n",
            "Epoch 37/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8088 - acc: 0.4352 - val_loss: 0.8370 - val_acc: 0.4373\n",
            "Epoch 38/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8084 - acc: 0.4363 - val_loss: 0.8369 - val_acc: 0.4357\n",
            "Epoch 39/100\n",
            "227447/227447 [==============================] - 10s 46us/step - loss: 0.8082 - acc: 0.4354 - val_loss: 0.8365 - val_acc: 0.4395\n",
            "Epoch 40/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8080 - acc: 0.4357 - val_loss: 0.8362 - val_acc: 0.4402\n",
            "Epoch 41/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8076 - acc: 0.4357 - val_loss: 0.8365 - val_acc: 0.4379\n",
            "Epoch 42/100\n",
            "227447/227447 [==============================] - 10s 42us/step - loss: 0.8076 - acc: 0.4354 - val_loss: 0.8354 - val_acc: 0.4409\n",
            "Epoch 43/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8073 - acc: 0.4358 - val_loss: 0.8355 - val_acc: 0.4393\n",
            "Epoch 44/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8074 - acc: 0.4359 - val_loss: 0.8360 - val_acc: 0.4407\n",
            "Epoch 45/100\n",
            "227447/227447 [==============================] - 10s 42us/step - loss: 0.8071 - acc: 0.4368 - val_loss: 0.8367 - val_acc: 0.4379\n",
            "Epoch 46/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8071 - acc: 0.4360 - val_loss: 0.8352 - val_acc: 0.4408\n",
            "Epoch 47/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8069 - acc: 0.4359 - val_loss: 0.8352 - val_acc: 0.4371\n",
            "Epoch 48/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8067 - acc: 0.4358 - val_loss: 0.8348 - val_acc: 0.4396\n",
            "Epoch 49/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8067 - acc: 0.4362 - val_loss: 0.8349 - val_acc: 0.4407\n",
            "Epoch 50/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8066 - acc: 0.4354 - val_loss: 0.8344 - val_acc: 0.4413\n",
            "Epoch 51/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8065 - acc: 0.4359 - val_loss: 0.8347 - val_acc: 0.4392\n",
            "Epoch 52/100\n",
            "227447/227447 [==============================] - 10s 45us/step - loss: 0.8065 - acc: 0.4353 - val_loss: 0.8346 - val_acc: 0.4394\n",
            "Epoch 53/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8064 - acc: 0.4351 - val_loss: 0.8346 - val_acc: 0.4364\n",
            "Epoch 54/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8062 - acc: 0.4357 - val_loss: 0.8343 - val_acc: 0.4389\n",
            "Epoch 55/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8061 - acc: 0.4357 - val_loss: 0.8346 - val_acc: 0.4367\n",
            "Epoch 56/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8060 - acc: 0.4358 - val_loss: 0.8348 - val_acc: 0.4359\n",
            "Epoch 57/100\n",
            "227447/227447 [==============================] - 10s 42us/step - loss: 0.8062 - acc: 0.4348 - val_loss: 0.8343 - val_acc: 0.4370\n",
            "Epoch 58/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8059 - acc: 0.4352 - val_loss: 0.8340 - val_acc: 0.4363\n",
            "Epoch 59/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8060 - acc: 0.4350 - val_loss: 0.8338 - val_acc: 0.4372\n",
            "Epoch 60/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8058 - acc: 0.4353 - val_loss: 0.8346 - val_acc: 0.4390\n",
            "Epoch 61/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8058 - acc: 0.4349 - val_loss: 0.8343 - val_acc: 0.4381\n",
            "Epoch 62/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8058 - acc: 0.4349 - val_loss: 0.8337 - val_acc: 0.4369\n",
            "Epoch 63/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8057 - acc: 0.4351 - val_loss: 0.8342 - val_acc: 0.4315\n",
            "Epoch 64/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8056 - acc: 0.4346 - val_loss: 0.8341 - val_acc: 0.4373\n",
            "Epoch 65/100\n",
            "227447/227447 [==============================] - 10s 42us/step - loss: 0.8056 - acc: 0.4342 - val_loss: 0.8345 - val_acc: 0.4297\n",
            "Epoch 66/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8055 - acc: 0.4348 - val_loss: 0.8338 - val_acc: 0.4385\n",
            "Epoch 67/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8055 - acc: 0.4346 - val_loss: 0.8334 - val_acc: 0.4388\n",
            "Epoch 68/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8054 - acc: 0.4342 - val_loss: 0.8345 - val_acc: 0.4361\n",
            "Epoch 69/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8054 - acc: 0.4335 - val_loss: 0.8334 - val_acc: 0.4374\n",
            "Epoch 70/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8053 - acc: 0.4345 - val_loss: 0.8329 - val_acc: 0.4357\n",
            "Epoch 71/100\n",
            "227447/227447 [==============================] - 10s 46us/step - loss: 0.8053 - acc: 0.4343 - val_loss: 0.8338 - val_acc: 0.4354\n",
            "Epoch 72/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8053 - acc: 0.4343 - val_loss: 0.8337 - val_acc: 0.4366\n",
            "Epoch 73/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8052 - acc: 0.4343 - val_loss: 0.8332 - val_acc: 0.4354\n",
            "Epoch 74/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8052 - acc: 0.4343 - val_loss: 0.8335 - val_acc: 0.4363\n",
            "Epoch 75/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8052 - acc: 0.4342 - val_loss: 0.8334 - val_acc: 0.4379\n",
            "Epoch 76/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8053 - acc: 0.4341 - val_loss: 0.8332 - val_acc: 0.4360\n",
            "Epoch 77/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8050 - acc: 0.4344 - val_loss: 0.8330 - val_acc: 0.4363\n",
            "Epoch 78/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8051 - acc: 0.4335 - val_loss: 0.8328 - val_acc: 0.4379\n",
            "Epoch 79/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8050 - acc: 0.4340 - val_loss: 0.8333 - val_acc: 0.4339\n",
            "Epoch 80/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8050 - acc: 0.4339 - val_loss: 0.8327 - val_acc: 0.4371\n",
            "Epoch 81/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8049 - acc: 0.4344 - val_loss: 0.8329 - val_acc: 0.4373\n",
            "Epoch 82/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8050 - acc: 0.4340 - val_loss: 0.8328 - val_acc: 0.4367\n",
            "Epoch 83/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8049 - acc: 0.4340 - val_loss: 0.8329 - val_acc: 0.4373\n",
            "Epoch 84/100\n",
            "227447/227447 [==============================] - 11s 47us/step - loss: 0.8049 - acc: 0.4337 - val_loss: 0.8329 - val_acc: 0.4339\n",
            "Epoch 85/100\n",
            "227447/227447 [==============================] - 10s 44us/step - loss: 0.8048 - acc: 0.4343 - val_loss: 0.8328 - val_acc: 0.4382\n",
            "Epoch 86/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8047 - acc: 0.4343 - val_loss: 0.8357 - val_acc: 0.4285\n",
            "Epoch 87/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8048 - acc: 0.4336 - val_loss: 0.8325 - val_acc: 0.4321\n",
            "Epoch 88/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8047 - acc: 0.4338 - val_loss: 0.8338 - val_acc: 0.4311\n",
            "Epoch 89/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8047 - acc: 0.4339 - val_loss: 0.8325 - val_acc: 0.4382\n",
            "Epoch 90/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8046 - acc: 0.4344 - val_loss: 0.8326 - val_acc: 0.4363\n",
            "Epoch 91/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8046 - acc: 0.4340 - val_loss: 0.8328 - val_acc: 0.4354\n",
            "Epoch 92/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8046 - acc: 0.4338 - val_loss: 0.8324 - val_acc: 0.4360\n",
            "Epoch 93/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8047 - acc: 0.4339 - val_loss: 0.8325 - val_acc: 0.4383\n",
            "Epoch 94/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8047 - acc: 0.4340 - val_loss: 0.8327 - val_acc: 0.4371\n",
            "Epoch 95/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8045 - acc: 0.4343 - val_loss: 0.8325 - val_acc: 0.4363\n",
            "Epoch 96/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8046 - acc: 0.4341 - val_loss: 0.8332 - val_acc: 0.4358\n",
            "Epoch 97/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8045 - acc: 0.4340 - val_loss: 0.8322 - val_acc: 0.4379\n",
            "Epoch 98/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8045 - acc: 0.4341 - val_loss: 0.8326 - val_acc: 0.4369\n",
            "Epoch 99/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8045 - acc: 0.4342 - val_loss: 0.8321 - val_acc: 0.4373\n",
            "Epoch 100/100\n",
            "227447/227447 [==============================] - 10s 43us/step - loss: 0.8045 - acc: 0.4340 - val_loss: 0.8330 - val_acc: 0.4352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44mkfINQk6ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eigj1-TS3b-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = autoencoder.predict(X_test) #Making prediction for X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YdJGy-94B9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mse = np.mean(np.power(X_test - y_pred,2),axis=1) #This is the formula for the mean squared error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvIX-P9B4ON3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error_df = pd.DataFrame({'reconstruction_error':mse,'true_class':y_test}) #Making a dataframe for the reconstruction error and the actual input "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1R-x4FZ4hA8",
        "colab_type": "code",
        "outputId": "11d296fd-95d8-4903-d74b-eda8d9b65310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "error_df.describe()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reconstruction_error</th>\n",
              "      <th>true_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>56962.000000</td>\n",
              "      <td>56962.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.832993</td>\n",
              "      <td>0.00165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.333125</td>\n",
              "      <td>0.04059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.095779</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.316677</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.501478</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.752839</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>383.794288</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       reconstruction_error   true_class\n",
              "count          56962.000000  56962.00000\n",
              "mean               0.832993      0.00165\n",
              "std                3.333125      0.04059\n",
              "min                0.095779      0.00000\n",
              "25%                0.316677      0.00000\n",
              "50%                0.501478      0.00000\n",
              "75%                0.752839      0.00000\n",
              "max              383.794288      1.00000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WThrZFi4i1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,precision_recall_curve,classification_report,roc_curve,auc #Importing tools for visualizing the test results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9JZiF6Hp6mN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roc_auc = auc(fpr,tpr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HusZFso40_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr,thresholds = roc_curve(error_df['true_class'],error_df['reconstruction_error'])#This is the roc_curve. It can be seen lower down."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpMPrwK5gAt",
        "colab_type": "code",
        "outputId": "ab021b89-fe65-4bcd-fc99-e2a97df12ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(fpr,tpr,label='AUC = %0.4f' % roc_auc)\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.xlim([-0.001, 1])\n",
        "plt.ylim([0, 1.001])\n",
        "plt.ylabel('True positive')\n",
        "plt.xlabel('False positive')\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZgU5bXH8e8RxSUCSsBIWAQjXlkU\nwQmgxohxwyWoCYIoEQUU1Hg1LlHjEjWuqMQNBVSUYFBQYySK0YQYFxCEyD4umaBR1BtRUFR2OPeP\ntybTDDM9PcNUVy+/z/PM093VNdWHEjl96q33vObuiIiIVGebpAMQEZHcpkQhIiJpKVGIiEhaShQi\nIpKWEoWIiKSlRCEiImnFlijMbJyZfWpmi6p538zsbjMrM7MFZtYtrlhERKTu4qwoHgF6p3n/GKB9\n9HM2cH+MsYiISB3Flijc/RVgeZpdTgB+58FMYBczaxFXPCIiUjfbJvjZLYEPU14vjbZ9ku6XmjVr\n5m3bto0xLBHJ1PJv1vHFqvVJhyFp7LT6K3ZZuZzFa1d/5u7N63KMJBNFxszsbMLlKdq0acOcOXMS\njkhEAPqPeZ3ST1bSsUXjpEORytzBjO8sW8rAp0bRY97L/67roZJMFB8BrVNet4q2bcHdxwJjAUpK\nStScSiRGE2d9wDPzqvxfcQvlSWLSsANjjkoytn493HYbzJsHkyaBGVx1cnisoyRvj50CnB7d/dQT\n+NLd0152EpH4PTPvI0o/WZnRvh1bNOaE/VvGHJFk7M034fvfhyuvDIlh7dp6OWxsFYWZPQb0ApqZ\n2VLg18B2AO4+GpgKHAuUAauAM+OKRUSqVlX1oCohD61eDdddB7ffDs2bw9NPw4kn1tvhY0sU7j6g\nhvcdOC+uzxeRmpVXD6ljDKoS8tCqVfDIIzBoUEgWu+5ar4fPi8FsEYmPqoc8tXIljBoFl14K3/42\nLF4cHmOgRCFFozaDtMVCdyzlqeefh2HDYOlS6N4dDj88tiQB6vUkRaQ2g7TFQpeZ8sznn8Ppp8Ox\nx0KjRjBjRkgSMVNFIbHJtW/wGqSVvHfyyfDqq3D11eHOpu23z8rHKlFIbKoaKE2Svj1LXvr441A9\nNGoEd9wBDRrAfvtlNQQlCtkq6aoGfYMX2QruMG4cXHxxuJvprruga9dEQtEYhWyVdNf99Q1epI6W\nLIEjj4ShQ2H//eH88xMNRxVFAcrm2ICqBpF69vTTMHBguMQ0ejScdRZsk+x3elUUBSibd/eoahCp\nJx61sevSBY45JsyLGDYs8SQBqigKlr7li+SJdevg1ltDE78nn4Q99wyPOUSJYivl2i2goElUInlj\n9mwYMgQWLoQBA0ITvx12SDqqLSRf0+S5XJzEpctBIjlu9Wr45S+hZ09YvhymTIGJE3MySYAqCmDr\nqgIN5opIra1eDRMmhLuaRoyAJk2SjigtVRRsXVWgb+8ikpGVK+HGG2HDBmjaFEpLYcyYnE8SUKAV\nRW0rBFUFIhKr556D4cPDLOsDD4Qf/ajeW4HHqSAritpWCKoKRCQWy5bBaafB8cfDLrvA66+HJJFn\nCrKiAN0eKiI5oF8/mD4drr0WrrgCGjZMOqI6KbhEMXHWB8x6bzk92jVNOhQRKUZLl0LjxuFn5EjY\nbjvo3DnpqLZKwV16Kh+b0KUkEcmqTZtg7Fjo1Amuuips69o175MEFFiiSK0mTu3RJulwRKRYlJWF\nBYSGDYMDDoALLkg6onpVUIlC1YSIZN1TT4X1Id58Ex54AKZNg+99L+mo6lXBJApVEyKSVeVN/Lp2\nhT59wryIoUPBLNm4YlAwiULVhIhkxbp14S6mvn1DsthzT3j8cWhZuP/2FEyiAFRNiEi83ngDunWD\n666DHXcMTfyKQEElChGRWKxaFZYkPfBA+PJLePZZePTRnG3iV9+UKEREarJmDTz2GJx9dlhQ6Ljj\nko4oqwoiUZQPZIuI1JsvvoDrr4f16yua+N1/f5hIV2QKIlFoIFtE6tUzz0DHjmEs4rXXwrZddkk2\npgQVRKIADWSLSD349FM45RQ48URo3hxmzYLDDks6qsQVXK8nEZE6698fZsyA3/wGLrss9GkSJQoR\nKXIffhgWD2rcGO68MySHjh2TjiqnFMylJxGRWtm0KQxOd+oEV14ZtnXpoiRRBSUKESk+//xnGHs4\n91zo0QMuuijpiHKaEoWIFJfyJn7z58NDD8GLL0K7dklHldOUKESkOJQ38evWDU46KcyLGDy4IJv4\n1bdYE4WZ9Tazd8yszMwur+L9Nmb2kpnNNbMFZnZsnPGISBFauxauvjokB/dQPUycCN/9btKR5Y3Y\nEoWZNQBGAccAHYEBZlZ5lOgqYLK7dwVOAe6LKx4RKUKvvx7agN9wQ7izqUia+NW3OCuK7kCZuy9x\n93XA48AJlfZxoHw+fBPg49p+iNp3iMgWvvkGLrwQDj44PH/+eRg/vmia+NW3OBNFS+DDlNdLo22p\nrgUGmtlSYCpwflUHMrOzzWyOmc1ZtmzZZu+pfYeIbGHdOnjiCTjvPFi0CHr3TjqivJb0YPYA4BF3\nbwUcC0wwsy1icvex7l7i7iXNmzff4iBq3yEirFgRFhRavx523RXeegvuuQcaNUo6srwXZ6L4CGid\n8rpVtC3VEGAygLu/DuwANIsxJhEpRE8/HSbK3XBDRRO/IuzyGpc4E8VsoL2ZtTOzhoTB6imV9vkA\nOBzAzDoQEsUyREQy8Z//QL9+8JOfwO67hxXo1MSv3sWWKNx9A/Bz4AXgLcLdTYvN7Hoz6xPtdjFw\nlpnNBx4DznAvv9m5ZhrIFily/fvDlClw000Vy5RKvYu1KaC7TyUMUqduuybleSlwcF2Pr4FskSL0\n73+HtSGaNIG77oLtt4d99kk6qoKW9GD2VtNAtkiR2LQJ7r03NPG76qqwrUsXJYksUJtxEcl977wD\nQ4bA9Olw9NFwySVJR1RU8r6iEJECN3lyqBxKS+GRR8LkuT32SDqqoqJEISK5adOm8Ni9O/TtGxLF\noEFq4pcAJQoRyS1r1sCvfhXWrXaHtm3h0UfD7a+SCCUKEckd06fD/vvDzTdDs2ahFYckTolCRJL3\n9ddw/vlwyCGhonjhBRg3Ltz6KolTohCR5G3YENpwnH9+aOJ31FFJRyQpdHusiCRj+XL47W/hmmvC\nBLq33lIDvxylikJEsu/JJ6FDB7jlFpgxI2xTkshZeZso1OdJJA998gn89Kdw8snQujXMmQOHHpp0\nVFKDvL30pD5PInlowACYNQtuvRUuugi2zdt/gopKXv9XUp8nkTzw/vthIaEmTcJCQttvD3vvnXRU\nUgt5e+lJRHLcxo1w992bN/Hbd18liTyU1xWFiOSot96CoUPDQHXv3nDppUlHJFtBFYWI1K/Jk8Ps\n6rffhgkTYOpUaKNLxPlMiUJE6kdqE79TTglVxcCBauJXAJQoRGTrrF4Nl10GffpUNPEbPx522y3p\nyKSeKFGISN298kpYK2LECGjRQk38CpQShYjU3ldfwbnnhslyGzbAX/8KDzygJn4FSolCRGpv40Z4\n9ln4xS9g4UI4/PCkI5IYKVGISGY+/xyuvBLWrw9N/EpLYeRI+Na3ko5MYqZEISLpuYdbXjt0CGMR\n5U38dt452bgka5QoRKR6H38MJ50E/fvDHnvAP/6hJn5FSDOzRaR6AwbAG2/AbbfBhReqiV+R0n91\nEdnckiXQtGkYhxg1CnbYAfbaK+moJEG69CQiwcaNYcW5zp0rmvh17qwkIaooRISwTvWQIeEy0/HH\nw+WXJx2R5JCMKgoza2Vmh0XPtzcz3Q8nUigeewy6dQuXnCZOhClToFWrpKOSHFJjojCzwcAU4MFo\n0x7AM3EGJSJZsHFjeDzoIDjttDAvYsAANfGTLWRSUfwv0BNYCeDu7wLq9iWSr1atCutDlDfx22MP\nePhhaN486cgkR2WSKNa4+387fZlZA0BfOUTy0d//Hpr43X47tG6tJn6SkUwSxXQz+yWwQzROMQl4\nNt6wRKReffUVDB8Ohx0Wqoi//Q1Gj1YTP8lIJonil8BXwNvABcA04Mo4gxKRerZpEzz/PFxyCSxY\nEBKGSIYySRTHAQ+6+0nufqK73+/umzI5uJn1NrN3zKzMzKq8387M+plZqZktNrOJtQleRNJYtizc\n5rpuHTRpEgarb7sNdtop6cgkz2SSKE4Gyszs4egf/gaZHDjabxRwDNARGGBmHSvt0x64AjjY3TsB\nF9YqehHZknu4zbVDh9DddebMsF1dXqWOakwU7v4zYG/gT8CZwBIzG53BsbsDZe6+JBoMfxw4odI+\nZwGj3H1F9Fmf1iZ4Ealk6dJwN9Npp4UZ1XPnwg9/mHRUkucymnDn7msJcyceAWYD/TL4tZbAhymv\nl0bbUu0N7G1m081sppn1rupAZna2mc0xsznLli1j4qwPmPXe8kxCFykup50G06aFSmL6dOjUKemI\npADU2MLDzI4E+gNHAK8BvwNOrcfPbw/0AloBr5jZvu7+RepO7j4WGAtQUlLiz8z7CIAT9q+cd0SK\nUFkZNGsWmvjddx/suCPsuWfSUUkByaSiOBv4M9DB3Qe6+5TUeRVpfAS0TnndKtqWaikwxd3Xu/t7\nwLuExFGjHu2acmqPNpnsKlKYNmyAO+6A/faraOLXqZOShNS7TMYoTnb3J919dS2PPRtob2btzKwh\ncAqhFUiqPxKqCcysGeFS1JJafo5I8Vm4MLTeuOQSOPJIuOKKpCOSAlZtojCzl6PHFWa2POVnhZnV\nOEDg7huAnwMvAG8Bk919sZldb2Z9ot1eAD43s1LgJeBSd/98a/9QIgWtvInf++/D44/DH/8ILXUZ\nVuKTboyifEZOs7oe3N2nAlMrbbsm5bkDF0U/IpLOxo3QoAEcfDAMGgS33BLGJkRiVm1FkTKp7iF3\n35j6AzyUnfBEhG++gYsuCutEuEObNvDgg0oSkjWZDGbvl/oimkj3/XjCEZHNTJsG++4bVp5r105N\n/CQR6cYoLjOzFcB+qeMTwDIqXU4SkXq2ciUMHQpHHAHbbhu6vt53n5r4SSLSVRQjgObAb6PH5kAz\nd2/q7pdmIziRouUOf/0rXHYZzJ8Phx6adERSxNINZu/l7v80swnAf6d3WrT6lbsviDk2keLy6adh\nnYgbbqho4qcGfpID0iWKy4EhhMZ+lTmgBjIi9cEdfv97uOAC+Ppr+PGP4ZBDlCQkZ1SbKNx9SPR4\nSPbCESkyH3wQFhR6/nno2RMeegg6dqz590SyqMa7nszsJ2bWKHp+uZlNNrMu8YcmUgQGDoSXX4Y7\n74TXXlOSkJxUY1NA4Fp3/4OZHQQcC9wBjAF6xhqZSKF6911o3hx23TUsR7rjjuHWV5Eclck8io3R\n4/HAGHd/BtA9eiK1tWEDjBgBXbrA1VeHbR07KklIzsukovjEzMpXqjsgavCX0ToWIhKZPx8GD4Y3\n34STToIrtey85I9M/sHvB7wMHButRNeMcEeUiGRi4kQoKQmrzz3xBDz1FLRokXRUIhnLpM3418Bi\noJeZDQd2dffnY49MJN9t2BAeDzkkVBNvvQV9+0I0F0kkX2Ry19PPgSeANtHPZDM7N+7ARPLW11+H\nORHlTfxat4YxY6Bp06QjE6mTTFe46+7uv3L3XwE9gOHxhiWSp/7yl9DE7+67oX17WL8+6YhEtlom\nicKA1JaV66NtIlLuyy/D5aWjjgqN+159Fe65Bxo2TDoyka2WyV1PE4BZZvYUIUGcCIyPNSqRfGMG\nL70UliS95hrYYYekIxKpNzUmCncfYWZ/B35A6PE03N1nxx2YSM77v/8LTfxuugkaNw5N/HbcMemo\nROpdpvMh1gBrUx5Fipc7jB8fJsvdey/MmhW2K0lIgcrkrqcrgceAFkArYKKZXRF3YCI56d//hmOO\ngTPOCIli3rxw+6tIActkjOJ0oKu7rwIwsxuBucDNcQYmkpNOPz3Mrr73XjjnHNhGTQqk8GXUwqPS\nfttG20SKwzvvwG67VTTx22kn2GOPpKMSyZpMvg4tBxab2YNm9gCwEPjMzEaa2ch4wxNJ0Pr1cPPN\nmzfx69BBSUKKTiYVxXPRT7mZMcUikjvmzg3zIubNC203rroq6YhEEpPJ7bEPZSMQkZzx6KNhsLp5\n89DA7yc/SToikURpJE6kXHkTv0MPhbPOCvMilCRElChE+OorOP98OO64iiZ+998fBq9FJPNEYWZa\n1U4Kz5//DJ07w6hRsM8+auInUoVMJtx1N7OFwD+j113M7J7YIxOJ0xdfwKBBYfLcTjvBa6/BXXep\niZ9IFTKpKO4mrJf9OYC7zwcOizMokdhts01IDldfHe5sOuigpCMSyVmZ3B67jbv/2zZflWtjTPGI\nxOeTT2DECLjlltDEb/FidXkVyUAmFcWHZtYdcDNrYGYXAu/GHJdI/XGHhx8OvZlGj4bZUfNjJQmR\njGSSKM4BLiIsg/ofoGe0TST3vfdeWExo8OCw8tz8+fCDHyQdlUheyWTC3afAKVmIRaT+DRoUZlnf\ndx8MG6YmfiJ1UGOiiPo7eeXt7n52LBGJbK233oLddw/zIMaODXc1tWmTdFQieSuTr1d/BaZFP9OB\n3chw8SIz621m75hZmZldnma/n5qZm1lJJscVqdL69XDDDbD//hVN/PbZR0lCZCtlculpUuprM5sA\nvFbT75lZA2AUcCSwFJhtZlPcvbTSfo2AC4BZtYhbZHP/+EcYh1iwAPr1C+tWi0i9qMsF23bAdzLY\nrztQ5u5L3H0d8DhwQhX7/Qa4lbDMqkjtTZgA3bvDsmXw9NMwaVJYP0JE6kUmM7NXmNny6OcL4C9A\nJkuhtgQ+THm9NNqWeuxuQGt3T21jXlUMZ5vZHDObs2zZsgw+WopCebuNww6D4cNDE78TT0w2JpEC\nlPbSk4VZdl2Aj6JNm9x9i4HtujCzbYCRwBk17evuY4GxACUlJfXy+ZLHVq6Eyy6DsjJ48UVo1Sr0\nahKRWKStKKKkMNXdN0Y/tflH+iOgdcrrVlQkHIBGQGfg72b2PmF+xhQNaEtaU6dCp04wZkyYF6Em\nfiKxy2SMYp6Zda3DsWcD7c2snZk1JMzFmFL+prt/6e7N3L2tu7clrJzXx93npDvo8m/WMeu95XUI\nR/LaihUwcGBoBd64McyYASNHqomfSBZUe+nJzLZ19w1AV8IdS/8CvgGMUGx0S3dgd99gZj8HXgAa\nAOPcfbGZXQ/Mcfcp6X6/Ol+sWk9j4IT9W9a4rxSQBg1g5kz49a/hiitge3W9F8mWdGMUbwDdgD51\nPbi7TwWmVtpW5X2L7t4r0+P2aNeUU3vo3viC9/HHcOutoZFfeRM/JQiRrEuXKAzA3f+VpVhEAncY\nNw4uvhjWrg3zIg4+WElCJCHpEkVzM7uoujfdfWQM8UixW7IkrFf9t7+FtasffBD22ivpqESKWrpE\n0QDYmaiyEMmKM84ICwmNGQNDh6qJn0gOSJcoPnH367MWiRSvxYuhRQto2jQ08dt55zA3QkRyQrqv\na6okJF7r1sF110HXrps38VOSEMkp6SqKw7MWhRSf2bNDE79Fi+DUU+Haa5OOSESqUW1F4e6a1Sbx\nGD8eevYMk+j+9Cf4/e+hefOkoxKRamikULKnvN3GEUfAeeeFsYnjj082JhGpkRKFxO/LL8MypL17\nhzkSLVvC3XdDkyZJRyYiGVCikHg9+2xo4vfgg9Ctm5r4ieQhJQqJx4oVYZD6xz8Oa1e//jrcdpua\n+InkISUKice224Y7m667LixT2r170hGJSB3VuGa2SMaWLoVbboE77oBGjcKtr+rPJJL3VFHI1tu0\nKbTc6NgRHn44VBCgJCFSIJQoZOuUlcHhh4c1q7t3h4UL4aCDko5KROqRLj3J1hk8GBYsCHc1DR4M\nps4vIoVGiUJqb+FC+O534dvfDgli553DaxEpSLr0JJlbuzYsRdqtW3gE2HtvJQmRAqeKQjIzcyYM\nGQKlpTBwYLjtVUSKgioKqdn48WGAeuVKeO45mDAhXHYSkaKgRCHVW7cuPB5xBPzv/4Ymfscem2xM\nIpJ1ShSypS++CMuQ9u4d5ki0bAl33gmNGycdmYgkQIlCNvfMM2Hi3COPhHkRGzYkHZGIJEyD2RIs\nXw7nnAOTJ0OXLmFBoQMOSDoqEckBqigkaNgQ5s2DG24IzfyUJEQkooqimH34Idx8M4wcGSbNLVyo\nNuAisgVVFMVo0ya4//4wFjF+PLz5ZtiuJCEiVVCiKDbvvgu9esG550LPnqEVuJr4iUgauvRUbIYO\nDZeYxo2DM85QEz8RqZESRTGYPx9atQqzqR96KIxHtGiRdFQikid06amQrVkDV10FJSUVTfzat1eS\nEJFaUUVRqGbMCE383n4bBg2C669POiIRyVOqKArRuHHwgx/AqlXw5z+HWdZNmyYdlYjkKSWKQrJ2\nbXjs3Rsuuijc0XT00cnGJCJ5L9ZEYWa9zewdMyszs8ureP8iMys1swVmNs3M9ogznoK1YgWceWZF\nE7/vfhduvx0aNUo6MhEpALElCjNrAIwCjgE6AgPMrGOl3eYCJe6+H/AkMCKueArWH/4QJs5NmAAH\nHggbNyYdkYgUmDgriu5Ambsvcfd1wOPACak7uPtL7r4qejkTaBVjPIXl88+hb1/46U9h991Df6ab\nboLttks6MhEpMHEmipbAhymvl0bbqjMEeL6qN8zsbDObY2Zz1q9fX48h5rHttw9jEDfdBG+8AV27\nJh2RiBSonBjMNrOBQAlwW1Xvu/tYdy9x95Ltivkb8/vvw7BhYX5EeRO/K65QFSEisYozUXwEtE55\n3SrathkzOwK4Eujj7mtjjCd/bdoE99wDnTvDxImhHTgoQYhIVsSZKGYD7c2snZk1BE4BpqTuYGZd\ngTGEJPFpjLHkr7ffhh/+MKxZfcgh4XJTz55JRyUiRSS2mdnuvsHMfg68ADQAxrn7YjO7Hpjj7lMI\nl5p2Bp6w0JzuA3fvE1dMeWnYMCgtDe3Af/YzNfETkayLtYWHu08Fplbadk3K8yPi/Py8NXcutG4N\nzZqFJn6NGsF3vpN0VCJSpHJiMLs2vlm3IekQ4rNmTRic/v73K5r47bWXkoSIJCovmwKesH+6u2zz\n1GuvhSZ+774bZlnfcEPSEYmIAHlYUXyr4bac2qNN0mHUr3HjwoD1unXw4ovh9a67Jh2ViAiQh4mi\noKxZEx5794ZLLgnzIo48MtmYREQqUaJIwuefhzUijj66oonfiBFhEp2ISI5Rosgmd3jiidDEb+JE\nOPRQNfETkZyXl4PZeemzz+Css+CPf4QDDghjEV26JB2ViEiNVFFkyw47wDvvhEtMM2cqSYhI3lCi\niNN774UqoryJ34IFcOmlsK0KORHJH0oUcdi4Ee66KzTxe/zxiiZ+ShAikoeUKOpbaWlo3nfhhWGw\nurRUTfxEJK/pK259Gz48zK5+9FE49VQ18RORvKdEUR/mzIG2bUMTv4cfDk38dtst6ahEROqFLj1t\njdWr4Ze/hB494Nprw7bvfU9JQkQKiiqKunr5ZRg6FMrKwp1NN96YdEQiIrFQRVEXDz4IvXqF9hvT\npsHYsdCkSdJRiYjEQomiNlavDo/HHQeXXx6a+P3oR8nGJCISMyWKTHz2GQwcGLq8btoELVrAzTfD\nTjslHZmISOyUKNJxh0mTQhO/yZPhsMPUxE9Eio4Gs6vz2WdhxbkpU8LSpA89BPvum3RUIiJZp4qi\nOjvuGO5ouv12mDFDSUJEipYSRap//QsGDw6D1t/6FsyfDxdfrB5NIlLUlCggjDuMHBmqhqeeCl1e\nQQlCRAQlCli0CA46KFQORxwRmvj16JF0VCIiOUNfmc87D5Ysgcceg/791cRPRKSS4kwUb7wB7dpB\n8+YVTfyaN086KhGRnFRcl55WrYJLLoEDD4Trrgvb9txTSUJEJI3iqSheeik08VuyBIYNUxM/EZEM\nFUdF8cADoSeTWUgYo0eriZ+ISIYKO1GsWhUef/xj+NWvwm2vvXolGpKISL4pzESxbBkMGABHHx2a\n+O2+e7jUpCZ+IiK1VliJwh0mToQOHcLEuaOOColCRETqrHAGs5ctgzPPhOeeg549w+JCnTolHZWI\nSN4rnIpip53ggw/gzjvhtdeUJERE6kmsicLMepvZO2ZWZmaXV/H+9mY2KXp/lpm1rdUHlJWFKqK8\nid/cuXDBBdCgQT39CUREJLZEYWYNgFHAMUBHYICZday02xBghbvvBfwWuDWjg2/YENp/77svPP10\nWJIUlCBERGIQZ0XRHShz9yXuvg54HDih0j4nAOOj508Ch5ulb7bUcP2aMLP60kvDXU2lpdC9e70H\nLyIiQZyD2S2BD1NeLwUqt2X97z7uvsHMvgS+DXxW3UGbLf8PbFwVlibt21dN/EREYpYXdz2Z2dnA\n2dHLtfbp6kX065dkSLmiGWmSapHRuaigc1FB56LC/9T1F+NMFB8BrVNet4q2VbXPUjPbFmgCfF75\nQO4+FhgLYGZz3L0klojzjM5FBZ2LCjoXFXQuKpjZnLr+bpxjFLOB9mbWzswaAqcAUyrtMwUYFD3v\nC/zN3T3GmEREpJZiqyiiMYefAy8ADYBx7r7YzK4H5rj7FOAhYIKZlQHLCclERERySKxjFO4+FZha\nads1Kc/XACfX8rBj6yG0QqFzUUHnooLORQWdiwp1PhemKz0iIpJO4bTwEBGRWORsooi9/UceyeBc\nXGRmpWa2wMymmdkeScSZDTWdi5T9fmpmbmYFe8dLJufCzPpFfzcWm9nEbMeYLRn8P9LGzF4ys7nR\n/yfHJhFn3MxsnJl9amaLqnnfzOzu6DwtMLNuGR3Y3XPuhzD4/S9gT6AhMB/oWGmfc4HR0fNTgElJ\nx53guTgM2Cl6fk4xn4tov0bAK8BMoCTpuBP8e9EemAvsGr3eLem4EzwXY4FzoucdgfeTjjumc/FD\noBuwqJr3jwWeBwzoCczK5Li5WlHE0v4jT9V4Ltz9JXePlvNjJmHOSiHK5O8FwG8IfcPWZDO4LMvk\nXJwFjHL3FQDu/mmWY8yWTM6FA42j502Aj7MYX9a4+yuEO0ircwLwOw9mAruYWYuajpuriaKq9h8t\nq9vH3TcA5e0/Ck0m5yLVEEdtyQMAAASwSURBVMI3hkJU47mISunW7v5cNgNLQCZ/L/YG9jaz6WY2\n08x6Zy267MrkXFwLDDSzpYQ7Mc/PTmg5p7b/ngB50sJDMmNmA4ES4NCkY0mCmW0DjATOSDiUXLEt\n4fJTL0KV+YqZ7evuXyQaVTIGAI+4+x1mdiBh/lZnd9cSmBnI1YqiNu0/SNf+owBkci4wsyOAK4E+\n7r42S7FlW03nohHQGfi7mb1PuAY7pUAHtDP5e7EUmOLu6939PeBdQuIoNJmciyHAZAB3fx3YgdAH\nqthk9O9JZbmaKNT+o0KN58LMugJjCEmiUK9DQw3nwt2/dPdm7t7W3dsSxmv6uHude9zksEz+H/kj\noZrAzJoRLkUtyWaQWZLJufgAOBzAzDoQEsWyrEaZG6YAp0d3P/UEvnT3T2r6pZy89ORq//FfGZ6L\n24CdgSei8fwP3L1PYkHHJMNzURQyPBcvAEeZWSmwEbjU3Quu6s7wXFwMPGBmvyAMbJ9RiF8szewx\nwpeDZtF4zK+B7QDcfTRhfOZYoAxYBZyZ0XEL8FyJiEg9ytVLTyIikiOUKEREJC0lChERSUuJQkRE\n0lKiEBGRtJQopCCY2UYzm5fy0zbNvm2r666ZbWZWYmZ3R897mdlBKe8NN7PTk4tOJMjJeRQidbDa\n3fdPOojaiiYDlk8I7AV8DcyI3hudUFgim1FFIQUrqhxeNbM3o5+Dqtink5m9EVUhC8ysfbR9YMr2\nMWbWoIrffd/MRpjZwmjfvVI+929WsT5Im2j7yWa2yMzmm9kr0bZeZvZsVAENB34RfeYhZnatmV1i\nZvuY2RuV/lwLo+cHmNnLZvYPM3shk06gIrWlRCGFYseUy05PR9s+BY50925Af+DuKn5vOHBXVI2U\nAEujFg/9gYOj7RuB06r53C/dfV/gXuDOaNs9wHh33w/4fcrnXgMc7e5dgM1mzrv7+8Bo4Lfuvr+7\nv5ry3ttAQzNrF23qD0wys+2iz+rr7gcA44Ab058mkdrTpScpFFVdetoOuNfMyv+x37uK33sduNLM\nWgF/cPd/mtnhwAHA7Kglyo6EpFOVx1Iefxs9PxD4SfR8AjAiej4deMTMJgN/qM0fjtDQrj9wS/TY\nH/gfQhPEv0RxNgBq7NsjUltKFFLIfgH8B+hCqJ63WMjI3Sea2SzgOGCqmQ0jrP413t2vyOAzvJrn\nW+7oPtzMekSf9Q8zOyCzPwYAkwi9vP4QDuX/NLN9gcXufmAtjiNSa7r0JIWsCfBJtObAzwjfuDdj\nZnsCS9z9buAZYD9gGtDXzHaL9mlq1a9D3j/l8fXo+QwqmlSeBrwaHed77j7L3a8hdC5NbfcM8BWh\nVfoW3P1fhKroakLSAHgHaB6tr4CZbWdmnaqJU6TOlCikkN0HDDKz+cA+wDdV7NMPWGRm8wiXcX7n\n7qXAVcCLZrYA+AtQ3SDxrtE+FxAqGAirp50Zbf9Z9B7AbdHA9yJCMplf6Vh/Ak4qH8yu4rMmAQOp\nWFdhHaHF/q3Rn3EesMWAvcjWUvdYkTqysDhSibt/lnQsInFSRSEiImmpohARkbRUUYiISFpKFCIi\nkpYShYiIpKVEISIiaSlRiIhIWkoUIiKS1v8DdBJxChkU3qIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0vrEhQOd-Tt",
        "colab_type": "text"
      },
      "source": [
        "We can plot the roc_curve. This is great curve for the binary classifier and this is exactly why it is perfect for our problem. The blue line represents the outcome of our algorithm. If it is closer to the top left corner of the image,the closer it is to the actual input. It is not great but it is pretty good. We can tweak afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZP-Ne-f58--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_4NcMaQ6XKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAQGKVYu6iwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 2.7 #Setting the threshold to 2.7 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzzCtjg76uJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = [1 if e > threshold else 0 for e in error_df['reconstruction_error'].values] #If the y_pred is bigger than the threshold we initiated it is set to 1 otherwise it is set to 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF9kBoLN7Cpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(error_df['true_class'],y_pred) #This is the confusion matrix "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHc8MmsO7JNF",
        "colab_type": "code",
        "outputId": "ee757bec-58c8-4a36-add5-5e1a918d8eb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "cm #So the result is pretty good, with a little bit tweaking we could get it to becaome even more precise."
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[54791,  2077],\n",
              "       [    9,    85]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kakdN3lO7MdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}